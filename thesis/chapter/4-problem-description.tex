\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

%Technical introduction. Describe design problem setup and constraints.

\begin{document}
    \chapter{Problem Description}
    \label{ch:problem-description}
    
    \section{Distributed Computing}
    \label{sec:distributed-computing-problem-description}

    % TODO: This was copied from proposal report. Should revise for concision and/or only include application stuff that is strictly related to the above description.
    % TODO: Should explain what $D$, $f$ $\mathbf{M}$, $T$ (and $\tau$?) are in the context of federated learning.
    % ***
    % TODO: split this section into it's background content \& it's specific problem description content

    \textit{Distributed computing} is the technique of linking multiple computers together to form a network for sharing data and coordinating processing power. A computer network can be configured in either a \textit{decentralized} or \textit{centralized} way. In a decentralized network, all computers can connect and communicate with each other. There is no single controlling authority which dictates the flow of information through the network.

    A centralized network is configured with a hierarchical structure and is controlled by a central authoritative computer. The central authority can directly connect to all other computers and control their function. In distributed computing, this central authority is called an \textit{orchestrator}. The orchestrator can employ the compute resources of the entire network to perform large parallel computation. The other computers in the network which perform the computation are called \textit{workers}. Since the application of this project's implementation is parallel learning, its focus will be on centralized networks.

    A centralized distributed computing network can be modelled mathematically using the notion of a network that was established in ~\autoref{sec:mathematical-description}. Denoting a collection of $n$ computers as $\mathbf{V} = (v_1, v_2, ..., v_n)$ with computation rates $\mathbf{R} = (r_1, r_2, ..., r_n)$ and price per processor-hour $\mathbf{P} = (p_1, p_2, ..., p_n)$, each $v_i$ can be thought of as a computer with a computation rate $r_i$ and a price to employ of $p_i$. In a \textit{heterogeneous} computer network, computational capacity and price of compute will vary from machine to machine.

    The structure of the distributed computing network is represented as the directed graph $\mathbf{G} = (\mathbf{V}, \mathbf{A})$. Because in a centralized network only the orchestrator can command processing power from the other computers, the adjacency matrix $\mathbf{A}$ is zero-valued for any edge which does not connect to the orchestrator node. In other words, there are no connections between any two computers that are not the orchestrator for the purposes of distributed computing. This feature is in addition to the other assumptions made for $\mathbf{A}$ in~\autoref{sec:mathematical-description}.

    When the orchestrator uses the network to compute in parallel, it is deploying a job denoted $J = (f,D)$ which consists of a function $f$ to be computed on a dataset $D \subseteq \mathbb{X}$. Here $\mathbb{X}$ is the infinite set of all possible input datasets. The orchestrator deploys a partition of $D$ to each employed worker along with copies of the work function $f$ for computation. The workers each return their computed results to the orchestrator, and these are the constituents of the result set $Y$.

    In distributed computing, it is in the best interest of the network to maximize the number of jobs that can be sent over the network and minimize the price to use the network. Therefore, optimizing for low run-time and cost of computing is desirable.

    By modelling the distributed computing network with the mathematical definition for a network, the run time and cost can be defined using the \textit{completion time} and \textit{resource price} functions, respectively. Given a job $J = (f,D)$ which is to be sent to the network and computed in parallel, one can minimize these functions regarding all variations of partitions of $D$ and their allocations to workers. If this is done before making use of the network, jobs can then be deployed strategically to minimize for these values and maximize the network's efficiency.

    \section{Optimization}
    \label{sec:optimization-problem-description}

    We begin to define a parallel-learning distributed computing network with the concept of the \textit{orchestrator}, denoted $O$. The orchestrator has the following properties:

    \begin{itemize}
        
        \item A \textit{job}, $J$, consisting of the tuple $J = (D, f)$ with
             \begin{itemize}
                  \item Dataset $D = \{d_1, ..., d_n\}$, $\vert D\vert = n$. Each batch of data $d_i$, for $i = 1,...,n$, is of uniform size $P_d$
                  \item Work function $f: D \rightarrow \mathbf{M}$ where $\mathbf{M}$ is the output space. A given job will have a work function producing output of fixed size $P_m$
              \end{itemize}
        \item A fixed upper bound on \textit{job run time}, $T$
    \end{itemize}
    
    The orchestrator employs a set of \textit{learners} $\mathbf{L} = \{l_1, ..., l_k\}$, $\vert\mathbf{L}\vert = k$. Each learner has the following properties:
    
    \begin{itemize}
        \item A \textit{cost} $c_i$, that the learner charges in exchange for computing a single batch of data, $d_i$. We define $c = \{c_1,...,c_k\}$ as the set of all learner costs. 
        \item A rate of computation $r_i$, which describes the learner's computational capability in batches of data per second.
    \end{itemize}
    
    Given the parameters of the agents in the system, we define the set of \textit{slices} of the dataset $D$ to be $\boldsymbol{D} = \{D_1,...,D_k\}$ where $\boldsymbol{D}$ forms a disjoint partition of D. i.e.
    
    \[\bigcup\limits_{i=1}^k D_i = D \textrm{\qquad and \qquad} \bigcap\limits_{i=1}^k D_i = \emptyset\]

    The set of \textit{slice sizes} of $\boldsymbol{D}$ is then defined $X = \{x_1,...,x_k\}$ where
    
    \[x_i = \vert D_i \vert \textrm{ for } i = 1,...,k \textrm{\qquad and \qquad} \sum_{i=1}^k x_i = n\]
    
    We define a \textit{subjob} $J_i = (D_i,f)$ for $i = 1,...,k$. Computing all $k$ subjobs is equivalent to computing job $J$. This will be done by assigning one subjob to each of the $k$ learners.
    
    The goal of our system is to choose the partition $\boldsymbol{D}$ of $D$ such that the slice size of the subjob assigned to each unique learner minimizes the total cost to compute the job, subject to some constraints:

    \begin{itemize}
        \item The total time to compute job $J$ is no greater than the upper bound on training time, $T$. That is, each worker must compute their subjob within $T$ units of time.
        \item \textit{Maximum assigned work}, $s^{max}_i = r_i \times T$, is the maximum quantity of batches (or maximumum slice size) that can be assigned to and executed by $l_i$ within training time $T$. Because workers are assumed to be heterogeneous and each have a unique compute capability $r_i$, this quantity will differ for each worker and work function. The value of $r_i$ cannot generally be known with certainty, but modelling in a later section (TODO: Insert ref) will provide a method for estimating $s_i^{max}$. We define $s^{max} = {s_1^{max},...,s_k^{max}}$ as the set of all learner's maximum assigned work.
        \item \textit{Minimum assigned work}, $s^{min}$, sets the minimum number of batches (or minimum slice size) that must be assigned to any worker receiving non-zero batches; i.e. a worker must be assigned \textit{at least} $s^{min}$ slices, or 0 otherwise. This ensures that workers do not compute too few slices to be effective in an application like parallel learning. (TODO: Citation)
        \item \textit{Minimum worker assignment}, $\beta$, sets the minimum number of workers which must be assigned work, i.e.,
              \[\beta \leq \vert \{x_i : x_i > 0\} \vert \leq k\]
        This ensures sufficient fulfillment of the goal to parallelize the the machine learning application.
    \end{itemize}

    We formalize this as a constrained optimization problem. Batches of the data set are homogeneous, and hence it is only required to determine the slice size $x_i$ of each subjob assigned to each learner, which minimizes total fees paid out to the learners.
    
    The problem is formulated as follows.

    \begin{equation}
        \label{eq:optimization-objective-function}
        \min_{x_i \in \mathbb{Z}_{\geq 0}} \ \sum_{i=1}^k x_i c_i
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        x_i \in \{0\} \cup \{s^{min}, s^{min} + 1,...,s^{max}_i\} &\qquad \text{for} \ i=1,...,k \label{eq:optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:optimization-constraint-2} \\
        \beta \leq \vert \{x_i : x_i > 0\} \vert \label{eq:optimization-constraint-3}
    \end{align}
    
    \subsection{Optimization Problem Simplification}
    \label{ssec:optimization-problem-simplification}
    
     Due to the semi-continuity of the slice-size variable $x$ \eqref{eq:optimization-constraint-1}, as well as the implications of the positivity of $x$ \eqref{eq:optimization-constraint-3}, there are some steps that will be taken to simplify the optimization problem from how it has been defined thus far. This will help when it comes time to produce a solution. We can exploit the known upper and lower bounds to set a linear constraint despite the semi-continuity of $x$, as well as model the implication of the positivity of $x$~\cite{9_mixed_integer_optimization_mosek_modeling_cookbook_3.2.3_2021}. Making a set of binary variables $Z = \{z_1,...,z_k\}$ where
     
    \[z_i =
        \begin{cases}
            1 & \text{if $x_i > 0$}\\
            0 & \text{if $x_i = 0$}\\
        \end{cases}
    \]
     
     we can replace constraint \eqref{eq:optimization-constraint-1} as the double linear inequality:
     
     \begin{equation}
        \label{eq:simplified-optimization-constraint-1}
         s^{min}Z \leq X \leq s^{max}Z, \qquad z_i \in \{0,1\}
     \end{equation}
     
     Indeed, since we must have $x_i < s_i^{max}$, then $x_i > 0$ mandates $z_i = 1$, and since $x_i = 0$ implies $x_i < s^{min}$, then $x_i = 0$ requires $z_i = 0$. Indeed $Z$ effectively models the positivity of $X$ and thus we can also replace \eqref{eq:optimization-constraint-3} as
     
     \begin{equation}
         \label{eq:simplified-optimization-constraint-3}
         \beta \leq \sum_{i = 1}^k z_i
     \end{equation}
    
    With these simplified constraints, we finalize the definition of the optimization problem as follows:
    
    \begin{equation}
        \label{eq:optimization-objective-function-simplified}
        \textrm{minimize} \qquad c^TX
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        s^{min}z_i \leq x_i \leq s^{max}z_i, \qquad i=1,...,k \label{eq:simplified-optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:optimization-constraint-2} \qquad \qquad \qquad\\
        \beta \leq \sum_{i = 1}^k z_i \label{eq:simplified-optimization-constraint-3} \qquad \qquad \qquad
    \end{align}

    \section{Parallel Learning}
    \label{sec:parallel-learning-problem-description}

    
\end{document}
