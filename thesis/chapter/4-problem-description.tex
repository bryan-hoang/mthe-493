\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

% Technical introduction. Describe design problem setup and constraints.

\begin{document}
    \chapter{Problem Description}
    \label{ch:problem-description}
    
    \section{Distributed Computing}
    \label{sec:distributed-computing-problem-description}

    \todo{This was copied from proposal report. Should revise for concision and/or only include application stuff that is strictly related to the above description.}
    \todo{Should explain what $D$, $f$ $\mathbf{M}$, $T$ (and $\tau$?) are in the context of federated learning.}
    % ***
    \todo{Split this section into it's background content \& it's specific problem description content}

    \textit{Distributed computing} is the technique of linking multiple computers together to form a network for sharing data and coordinating processing power. A computer network can be configured in either a \textit{decentralized} or \textit{centralized} way. In a decentralized network, all computers can connect and communicate with each other. There is no single controlling authority which dictates the flow of information through the network.

    A centralized network is configured with a hierarchical structure and is controlled by a central authoritative computer. The central authority can directly connect to all other computers and control their function. In distributed computing, this central authority is called an \textit{orchestrator}. The orchestrator can employ the compute resources of the entire network to perform large parallel computation. The other computers in the network which perform the computation are called \textit{learners}. Since the application of this project's implementation is parallel learning, its focus will be on centralized networks.

    When the orchestrator uses the network to compute in parallel, it is deploying a job denoted $J = (f,D)$ which consists of a function $f$ to be computed on a dataset $D \subseteq \mathbb{X}$. Here $\mathbb{X}$ is the infinite set of all possible input datasets. The orchestrator deploys a partition of $D$ to each employed worker along with copies of the work function $f$ for computation. The workers each return their computed results to the orchestrator, and these are the constituents of the result set $Y$.

    In distributed computing, it is in the best interest of the network to maximize the number of jobs that can be sent over the network and minimize the price to use the network. Therefore, optimizing for low run-time and cost of computing is desirable.

    By modelling the distributed computing network with the mathematical definition for a network, the run time and cost can be defined using the \textit{completion time} and \textit{resource price} functions, respectively. Given a job $J = (f,D)$ which is to be sent to the network and computed in parallel, one can minimize these functions regarding all variations of partitions of $D$ and their allocations to workers. If this is done before making use of the network, jobs can then be deployed strategically to minimize for these values and maximize the network's efficiency.

    \section{Optimization}
    \label{sec:optimization-problem-description}

    We begin to define a parallel-learning distributed computing network with the concept of the \textit{orchestrator}, denoted $O$. The orchestrator has the following properties:

    \begin{itemize}
        
        \item A \textit{job}, $J$, consisting of the tuple $J = (D, f)$ with
             \begin{itemize}
                  \item Dataset $D = \{d_1, ..., d_n\}$, $\vert D\vert = n$. Each batch of data $d_i$, for $i = 1,...,n$, is of uniform size $P_d$
                  \item Work function $f: D \rightarrow \mathbf{M}$ where $\mathbf{M}$ is the output space. A given job will have a work function producing output of fixed size $P_m$
              \end{itemize}
        \item A fixed upper bound on \textit{job run time}, $T$
    \end{itemize}
    
    The orchestrator employs a set of \textit{learners} $\mathbf{L} = \{l_1, ..., l_k\}$, $\vert\mathbf{L}\vert = k$. Each learner has the following properties:
    
    \begin{itemize}
        \item A \textit{cost} $c_i$, that the learner charges in exchange for computing a single batch of data, $d_i$.
        \item A rate of computation $r_i$, which describes the learner's computational capability in batches of data per second.
    \end{itemize}
    
    Given the parameters of the agents in the system, we define the set of \textit{slices} of the dataset $D$ to be $\boldsymbol{D} = \{D_1,...,D_k\}$ where $\boldsymbol{D}$ forms a disjoint partition of D. i.e.
    
    \[\bigcup\limits_{i=1}^k D_i = D \textrm{\qquad and \qquad} \bigcap\limits_{i=1}^k D_i = \emptyset\]

    The set of \textit{slice sizes} of $\boldsymbol{D}$ is then defined $\mathbf{x} = \{x_1,...,x_k\}$ where
    
    \[x_i = \vert D_i \vert \textrm{ for } i = 1,...,k \textrm{\qquad and \qquad} \sum_{i=1}^k x_i = n\]
    
    We define a \textit{subjob} $J_i = (D_i,f)$ for $i = 1,...,k$. Computing all $k$ subjobs is equivalent to computing job $J$. This will be done by assigning one subjob to each of the $k$ learners.
    
    The goal of our system is to choose the partition $\boldsymbol{D}$ of $D$ such that the slice size of the subjob assigned to each unique learner minimizes the total cost to compute the job, subject to some constraints:

    \begin{itemize}
        \item The total time to compute job $J$ is no greater than the upper bound on training time, $T$. That is, each worker must compute their subjob within $T$ units of time.
        \item \textit{Maximum assigned work}, $s^{max}_i = r_i \times T$, is the maximum quantity of batches (or maximumum slice size) that can be assigned to and executed by $l_i$ within training time $T$. Because learners are assumed to be heterogeneous and each have a unique compute capability $r_i$, this quantity will differ for each learner and work function. The value of $r_i$ cannot generally be known with certainty, but modelling in a later section~\todo{Insert ref} will provide a method for estimating $s_i^{max}$.
        \item \textit{Minimum assigned work}, $s^{min}$, sets the minimum number of batches (or minimum slice size) that must be assigned to any learner receiving non-zero batches; i.e. a learner must be assigned \textit{at least} $s^{min}$ batches, or 0 otherwise. This ensures that learners do not compute too little data to be effective in an application like parallel learning~\todo{Citation}.
        \item \textit{Minimum learner assignment}, $\beta$, sets the minimum number of learners which must be assigned batches of data, i.e.,
              \[\beta \leq \vert \{x_i : x_i > 0\} \vert \leq k\]
        This ensures sufficient fulfillment of the goal to parallelize the the machine learning application.
    \end{itemize}

    We formalize this as a constrained optimization problem. Batches of the data set are homogeneous, and hence it is only required to determine the slice size $x_i$ of each subjob assigned to each learner, which minimizes total fees paid out to the learners.
    
    The problem is formulated as follows.

    \begin{equation}
        \label{eq:optimization-objective-function}
        \min_{x_i \in \mathbb{Z}_{\geq 0}} \ \sum_{i=1}^k x_i c_i
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        x_i \in \{0\} \cup \{s^{min}, s^{min} + 1,...,s^{max}_i\} &\qquad \text{for} \ i=1,...,k \label{eq:optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:optimization-constraint-2} \\
        \beta \leq \vert \{x_i : x_i > 0\} \vert \label{eq:optimization-constraint-3}
    \end{align}
    
    \subsection{Optimization Problem Simplification}
    \label{ssec:optimization-problem-simplification}
    
     Due to the semi-continuity of the slice-size variable \mathbf{x} \eqref{eq:optimization-constraint-1}, as well as the implications of the positivity of $\mathbf{x}$ \eqref{eq:optimization-constraint-3}, there are some steps that will be taken to simplify the optimization problem from how it has been defined thus far. This will help when it comes time to produce a solution.
     
     We can exploit the known upper and lower bounds to set a linear constraint despite the semi-continuity of \mathbf{x}, as well as model the implication of the positivity of $\mathbf{x}$~\cite{9_mixed_integer_optimization_mosek_modeling_cookbook_3.2.3_2021}. Making a set of binary variables $\mathbf{z} = \{z_1,...,z_k\}$ where
     
    \[z_i =
        \begin{cases}
            1 & \text{if $x_i > 0$}\\
            0 & \text{if $x_i = 0$}\\
        \end{cases}
    \]
     
     we can replace constraint \eqref{eq:optimization-constraint-1} as the double linear inequality:
     
     \begin{equation}
        % \label{eq:simplified-optimization-constraint-1}
         s^{min}z_i \leq x_i \leq s_i^{max}z_i, \qquad z_i \in \{0,1\}
     \end{equation}
     
     Indeed, since we must have $x_i < s_i^{max}$, having $x_i > 0$ mandates $z_i = 1$, and since $x_i = 0$ implies $x_i < s^{min}$, then $x_i = 0$ requires $z_i = 0$. Thus $\mathbf{z}$ does indeed effectively model the positivity of $\mathbf{x}$ and so we can also replace \eqref{eq:optimization-constraint-3} as
     
     \begin{equation}
        %  \label{eq:simplified-optimization-constraint-3}
         \beta \leq \sum_{i = 1}^k z_i
     \end{equation}
    
    With these simplified constraints, we finalize the definition of the optimization problem as follows:
    
    \begin{equation}
        \label{eq:optimization-objective-function-simplified}
        \min_{x_i \in \mathbb{Z}_{\geq 0}} \ \sum_{i=1}^k x_i c_i
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        s^{min}z_i \leq x_i \leq s^{max}z_i, \qquad i=1,...,k \label{eq:simplified-optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:simplified-optimization-constraint-2} \qquad \qquad \qquad\\
        \beta \leq \sum_{i = 1}^k z_i \label{eq:simplified-optimization-constraint-3} \qquad \qquad \qquad
    \end{align}

    The problem falls into the class of ILP problems (and into the greater class of MIP problems) since the objective function and constraints are linear and the decision variables are restricted to integrality. Appropriate methods to solving such a problem are explored later in \autoref{sec:optimization-engineering-tools}.

    \section{Parallel Learning}
    \label{sec:parallel-learning-problem-description}

    
\end{document}
