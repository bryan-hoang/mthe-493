\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

% Technical introduction. Describe design problem setup and constraints.

\begin{document}
    \chapter{Problem Description}
    \label{ch:problem-description}
    
    \section{Distributed Computing}
    \label{sec:distributed-computing-problem-description}

    From the context of distributed computing, the problem beings with a computer on a network, called the orchestrator as mentioned in~\autoref{sec:distributed-computing}. The orchestrator needs a computational task on a set of data to be completed that can be parallelized, namely a training task in parallel learning that will be referred to as a job. Given the benefit of speeding up the overall computations through distributed computing, the orchestrator needs to complete the job in a set amount of time, the global training time constraint of the system~\cite{yaqub_optimal_2020}.

    To lower the amount to time to complete the job, the orchestrator parallelizes the job by distributing the data and a task to computers on its network, namely learners. The learners, being edge devices representing consumer devices, naturally have different computational capabilities that affect the speed at which they can each complete the task on a subset of the data. So, the orchestrator will want to alter the amount of data allocated to each learner based on their computational capabilities to equalize the amount of time each learner spends working on the task.
    
    With that in mind, the orchestrator also has to consider that each learner has a cost of recruitment to do work that can also vary. Now the orchestrator is also aiming to minimize the overall cost it incurs by recruiting learners to do its work. But every learner needs to have some amount of data allocated to it, otherwise the parallel learning task will become less effective.

    Thus, from the perspective of the orchestrator, the problem it is trying to solve is to distribute the job such that the overall run time falls below a global training time constraint while also trying to minimize the cost it incurs from having to pay the learners based on their recruitment cost and ensuring enough data is allocated to each learner. Therefore, the problem can be formulated as a mathematical optimization problem.

    \section{Optimization}
    \label{sec:optimization-problem-description}

    We begin to define a parallel-learning distributed computing network with the concept of the \textit{orchestrator}, denoted $O$. The orchestrator has the following properties:

    \begin{itemize}
        
        \item A \textit{job}, $J$, consisting of the tuple $J = (D, f)$ with
             \begin{itemize}
                  \item Dataset $D = \{d_1, ..., d_n\}$, $\vert D\vert = n$. Each batch of data $d_i$, for $i = 1,...,n$, is of uniform size $P_d$
                  \item Work function $f: D \rightarrow \mathbf{M}$ where $\mathbf{M}$ is the output space. A given job will have a work function producing output of fixed size $P_m$
              \end{itemize}
        \item A fixed upper bound on \textit{job run time}, $T$
    \end{itemize}
    
    The orchestrator employs a set of \textit{learners} $\mathbf{L} = \{l_1, ..., l_k\}$, $\vert\mathbf{L}\vert = k$. Each learner has the following properties:
    
    \begin{itemize}
        \item A \textit{cost} $c_i$, that the learner charges in exchange for computing a single batch of data, $d_i$.
        \item A rate of computation $r_i$, which describes the learner's computational capability in batches of data per second.
    \end{itemize}
    
    Given the parameters of the agents in the system, we define the set of \textit{slices} of the dataset $D$ to be $\boldsymbol{D} = \{D_1,...,D_k\}$ where $\boldsymbol{D}$ forms a disjoint partition of D. i.e.
    
    \[\bigcup\limits_{i=1}^k D_i = D \textrm{\qquad and \qquad} \bigcap\limits_{i=1}^k D_i = \emptyset\]

    The set of \textit{slice sizes} of $\boldsymbol{D}$ is then defined $\mathbf{x} = \{x_1,...,x_k\}$ where
    
    \[x_i = \vert D_i \vert \textrm{ for } i = 1,...,k \textrm{\qquad and \qquad} \sum_{i=1}^k x_i = n\]
    
    We define a \textit{subjob} $J_i = (D_i,f)$ for $i = 1,...,k$. Computing all $k$ subjobs is equivalent to computing job $J$. This will be done by assigning one subjob to each of the $k$ learners.
    
    The goal of our system is to choose the partition $\boldsymbol{D}$ of $D$ such that the slice size of the subjob assigned to each unique learner minimizes the total cost to compute the job, subject to some constraints:

    \begin{itemize}
        \item The total time to compute job $J$ is no greater than the upper bound on training time, $T$. That is, each learner must compute their subjob within $T$ units of time.
        \item \textit{Maximum assigned work}, $s^{max}_i = r_i \times T$, is the maximum quantity of batches (or maximum slice size) that can be assigned to and executed by $l_i$ within training time $T$. Because learners are assumed to be heterogeneous and each have a unique compute capability $r_i$, this quantity will differ for each learner and work function. The value of $r_i$ cannot generally be known with certainty, but modelling in \autoref{ssec:benchmarking} will provide a method for estimating $s_i^{max}$.
        \item \textit{Minimum assigned work}, $s^{min}$, sets the minimum number of batches (or minimum slice size) that must be assigned to any learner receiving non-zero batches; i.e. a learner must be assigned \textit{at least} $s^{min}$ batches, or 0 otherwise. This ensures that learners do not compute too little data to be effective in an application like parallel learning.
        \item \textit{Minimum learner assignment}, $\beta$, sets the minimum number of learners which must be assigned batches of data, i.e.,
              \[\beta \leq \vert \{x_i : x_i > 0\} \vert \leq k\]
        This ensures sufficient fulfillment of the goal to parallelize the the machine learning application.
    \end{itemize}

    We formalize this as a constrained optimization problem. Batches of the data set are homogeneous, and hence it is only required to determine the slice size $x_i$ of each subjob assigned to each learner, which minimizes total fees paid out to the learners.
    
    The problem is formulated as follows.

    \begin{equation}
        \label{eq:optimization-objective-function}
        \min_{x_i \in \mathbb{Z}_{\geq 0}} \ \sum_{i=1}^k x_i c_i
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        x_i \in \{0\} \cup \{s^{min}, s^{min} + 1,...,s^{max}_i\} &\qquad \text{for} \ i=1,...,k \label{eq:optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:optimization-constraint-2} \\
        \beta \leq \vert \{x_i : x_i > 0\} \vert \label{eq:optimization-constraint-3}
    \end{align}
    
    \subsection{Optimization Problem Simplification}
    \label{ssec:optimization-problem-simplification}
    
     Due to the semi-continuity of the slice-size variable $\mathbf{x}$ \eqref{eq:optimization-constraint-1}, as well as the implications of the positivity of $\mathbf{x}$ \eqref{eq:optimization-constraint-3}, there are some steps that will be taken to simplify the optimization problem from how it has been defined thus far. This will help when it comes time to produce a solution.
     
     We can exploit the known upper and lower bounds to set a linear constraint despite the semi-continuity of $\mathbf{x}$, as well as model the implication of the positivity of $\mathbf{x}$~\cite{9_mixed_integer_optimization_mosek_modeling_cookbook_3.2.3_2021}. Making a set of binary variables $\mathbf{z} = \{z_1,...,z_k\}$ where
     
    \[z_i =
        \begin{cases}
            1 & \text{if $x_i > 0$}\\
            0 & \text{if $x_i = 0$}\\
        \end{cases}
    \]
     
     we can replace constraint \eqref{eq:optimization-constraint-1} as the double linear inequality:
     
     \begin{equation}
        % \label{eq:simplified-optimization-constraint-1}
         s^{min}z_i \leq x_i \leq s_i^{max}z_i, \qquad z_i \in \{0,1\}
     \end{equation}
     
     Indeed, since we must have $x_i < s_i^{max}$, having $x_i > 0$ mandates $z_i = 1$, and since $x_i = 0$ implies $x_i < s^{min}$, then $x_i = 0$ requires $z_i = 0$. Thus $\mathbf{z}$ does indeed effectively model the positivity of $\mathbf{x}$ and so we can also replace \eqref{eq:optimization-constraint-3} as
     
     \begin{equation}
        %  \label{eq:simplified-optimization-constraint-3}
         \beta \leq \sum_{i = 1}^k z_i
     \end{equation}
    
    With these simplified constraints, we finalize the definition of the optimization problem as follows:
    
    \begin{equation}
        \label{eq:optimization-objective-function-simplified}
        \min_{x_i \in \mathbb{Z}_{\geq 0}} \ \sum_{i=1}^k x_i c_i
    \end{equation}
    % \[  \]

    subject to constraints

    \begin{align}
        s^{min}z_i \leq x_i \leq s^{max}z_i, \qquad i=1,...,k \label{eq:simplified-optimization-constraint-1} \\
        \sum_{i = 1}^k x_i = n \label{eq:simplified-optimization-constraint-2} \qquad \qquad \qquad\\
        \beta \leq \sum_{i = 1}^k z_i \label{eq:simplified-optimization-constraint-3} \qquad \qquad \qquad
    \end{align}

    The problem falls into the class of ILP problems (and into the greater class of MIP problems) since the objective function and constraints are linear and the decision variables are restricted to integrality. Appropriate methods to solving such a problem are explored later in \autoref{sec:optimization-engineering-tools}.

    \section{Parallel Learning}
    \label{sec:parallel-learning-problem-description}
    As mentioned in the background, the focus of this projectâ€™s distributed learning application is parallel learning. The edge devices, or learners as they will be referred to throughout the report, are responsible for nothing outside returning the results of local training computations. Training on learners turns into a vanilla machine learning problem as there are no considerations outside optimizing the local model for accuracy to contribute positively to the global model. For the purposes of our project, the machine learning problem will be simplified as much as possible as our focus is on the optimal partition of a dataset \& the optimal aggregation of many unique models.

    In sticking with simplicity for the machine learning problem, the neural network architecture of choice for our project is \textit{MNIST 2NN} or a 3 layer fully connected network. For an activation function, we selected \textit{ReLu}, and to normalize class predictions, \textit{softmax} was used. This all came from the paper we modelled our aggregation technique after and will be discussed more later in the report. The dataset of choice was \textit{MNIST or Modified National Institute of Standards and Technology}. It consists of 28x28 images of handwritten digits, zero through nine. There are 60 000 training and 10 000 testing images all containing 784 pixels. 
    
    When attempting to optimally configure a distributed computing network for parallel learning, one must consider two important variables, network costs and computational capabilities. When there are differences in the network costs and computational capabilities of edge devices, this creates whatâ€™s called system heterogeneity. System heterogeneity makes creating an optimal configuration substantially more difficult. If heterogeneity is not addressed, distributed learning performance is limited by its weakest edge or slowest connection. There are two conventional approaches to circumvent heterogeneity. One can either vary the volume of data sent to each edge or vary the number of local training iterations performed on each edge. The latter is often less accurate due to a discrepancy created in edge gradients that is referred to as 'staleness'. Based upon the scope of the assigned problem, only the volume of data will be varied as staleness was identified as a non-factor.
    
    The notion of combining the work of many learners sounds like an easy task, but due to the complexity of neural networks, reaching convergence is made substantially more difficult. Each learner receives an entirely unique slice of the dataset, so every modelâ€™s set of unique parameters must be considered when computing each global update. As the volume of data to learners differs, an added challenge of prioritizing learner model parameters is created.
    
    To summarize, this portion of the problem is to optimally aggregate local learner models into a highly accurate global model.
\end{document}
