\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

% Background &/or literature review on 1. Math, 2. Distributed Computing, 3. Federated Learning.

\begin{document}
    \chapter{Background}
    \label{ch:background}

    \section{Distributed Computing}
    \label{sec:distributed-computing}

    \textit{Distributed computing} is the technique of linking multiple computers together to form a network for sharing data and coordinating processing power. A computer network can be configured in either a \textit{decentralized} or \textit{centralized} way. In a decentralized network, all computers can connect and communicate with each other. There is no single controlling authority which dictates the flow of information through the network~\cite{noauthor_distributed_nodate}.

    A centralized network is configured with a hierarchical structure and is controlled by a central authoritative computer. The central authority can directly connect to all other computers and control their function. In distributed computing, this central authority is called an \textit{orchestrator}. The orchestrator can employ the compute resources of the entire network to perform large parallel computation. The other computers in the network which perform the computation are called \textit{workers}. Since the application of this project's implementation is parallel learning, its focus will be on centralized networks, and we will refer to the the workers as \textit{learners}~\cite{yaqub_optimal_2020}.
    
    \section{Contemporary Optimization}
    \label{sec:contemporary-optimization}

    A review of contemporary optimization problem classifications are identified below, so as to provide context for the optimization problem as stated in the problem description, \autoref{sec:optimization-problem-description}

    
    \subsection{Constrained Optimization}
    \label{ssec:constrained-optimization}

    A constrained optimization problem is an optimization problem in which the objective function $f$ must be maximized or minimized subject to \textit{hard constraints} $g_i$ and $h_j$, for $i=1,...,n$ and $j=1,...,m$.
    \[ \min \ f(\mathbf{x}) \]
    subject to
    \begin{align*}
        g_i(\mathbf{x}) &= c_i    & &\text{for} \ i = 1,...,n \ \textit{equality constraints}    \\
        h_j(\mathbf{x}) &\geq d_j & &\text{for} \ j = 1,...,m \ \textit{inequality constraints}. \\
    \end{align*}

    \subsection{Convex Optimization}
    \label{ssec:convex-optimization}
    
    Convex optimization problems are a subclass of constrained optimization problems. A convex optimization problem is simply an optimization problem in which the objective function and constraints are \textit{convex}~\cite{boyd_vandenberghe_2009}, i.e., for any objective function or constraint $f(\mathbf{x})$, 
        
        \[ f(\alpha \mathbf{x} + \beta \mathbf{y}) \leq \alpha f(\mathbf{x}) + \beta f(\mathbf{y}) \]
        
    In general, constrained optimization is NP-hard, however many classes of convex optimization problems can be solved using algorithms which operate in polynomial time.
    
    \subsubsection{Linear Programming}
    \label{sssec:linear-programming}
    
    Linear programming is a mathematical modelling technique in which the objective function and constraints are linear functions, i.e. the decision variable $\mathbf{x}$ is not raised to any power besides 1~\cite{the_editors_of_encyclopaedia_britannica_2017}. Linear programming problems are a subclass of convex optimization problems.
    
    \subsubsection{Integer Linear Programming}
    
    \label{sssec:integer-linear-programming}

        An integer linear program (ILP) is a linear programming problem which is further constrained by an integrality restriction on the decision variables~\cite{integer_programming}. That is, each element $x_i$ of $\mathbf{x}$ is such that $x_i \in \mathbb{Z}$. ILP problems are non-convex and NP-complete, making them much more difficult to solve.
        
        
        The naive way to solve ILP problems is to perform LP-relaxation, and solve the problem without obeying the integrality constraint, before rounding the solution back to an integer. Though this can work, it can also produce suboptimal or even infeasible solutions.
        
        Other ways to solve ILP problems include using exact algorithms such as cutting-plane or branch-and-cut techniques, or formulating heuristic processes. Heuristic methods are necessary for intractable problems, but can be disadvantageous in that if they fail to find a solution, it cannot be known whether it is because there is no feasible one, or if it simply couldn't find one.
        
        Mixed integer linear programming or mixed integer programming (MIP) is the class of problems where some, but not necessarily all decision variables are restricted to integrality. The tools and methods for solving MIP problems are generally used on ILP problems since they are a subset of MIP problems. For that reason MIP and ILP may be used interchangeably in the later discussions of optimization method efficacy, but the meaning is the same.

    \section{Parallel Learning}
    \label{sec:parallel-learning-bg}
    In many of its applications like image recognition, forecasting, and clustering, \textit{Machine Learning } (ML) outperforms existing techniques. This has motivated the need to deploy computationally dense ML models on edge devices. One area of ML where substantial momentum has been gained is in \textit{Distributed Learning}. This is the method of training of a model by aggregating many local models which are trained on edge devices. \textit{Federated Learning} is a form distributed learning where data is stored on edge devices. In this case, the edge devices are typically where training data is collected, such as a sensor on a smartphone. The focus of this project will be \textit{Parallel Learning}, where data begins on a central orchestrator and distributes it to edge devices for training. Unlike in other forms of distributed learning, that is the extent of the edge deviceâ€™s involvement.
\end{document}
