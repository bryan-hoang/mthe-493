\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

%Background &/or literature review on 1. Math, 2. Distributed Computing, 3. Federated Learning.

\begin{document}
    \chapter{Background}
    \label{ch:background}

    \section{Contemporary Optimization}

    A review of contemporary optimization problem classifications are identified below, so as to provide context for the optimization problem as stated in the problem description. (TODO: Insert ref)

    \subsection{Convex Optimization}
    \label{ssec:Convex Optimization}

    \subsubsection{Linear Programming}
    \label{sssec:Linear Programming}

    \subsubsection{Integer Linear Programming}
    \label{sssec:Linear Programming}

    \subsection{Constrained Optimization}
    \label{ssec:Constrained Optimization}

    A constrained optimization problem is an optimization problem in which the objective function $f$ must be minimized subject to \textit{hard constraints} $g_i$ and $h_j$, for $i=1,...,n$ and $j=1,...,m$.
    \[ \min \ f(\mathbf{x}) \]
    subject to
    \begin{align*}
        g_i(\mathbf{x}) &= c_i    & &\text{for} \ i = 1,...,n \ \textit{equality constraints}    \\
        h_j(\mathbf{x}) &\geq d_j & &\text{for} \ j = 1,...,m \ \textit{inequality constraints}. \\
    \end{align*}

    jot notes:
    - write out explicitly what our objective function and (in)equality constraints are
    - demonstrate that our objective function is linear in $\mathbf{x}$

    \section{Distributed Computing}
    \label{sec:Distributed-Computing}

    TODO: This was copied from proposal report. Should revise for concision and/or only include application stuff that is strictly related to the above description.
    TODO: Should explain what $D$, $f$ $\mathbf{M}$, $T$ (and $\tau$?) are in the context of federated learning.
    
    ***
    TODO: split this section into it's background content & it's specific problem description content

    \textit{Distributed computing} is the technique of linking multiple computers together to form a network for sharing data and coordinating processing power. A computer network can be configured in either a \textit{decentralized} or \textit{centralized} way. In a decentralized network, all computers can connect and communicate with each other. There is no single controlling authority which dictates the flow of information through the network.

    A centralized network is configured with a hierarchical structure and is controlled by a central authoritative computer. The central authority can directly connect to all other computers and control their function. In distributed computing, this central authority is called an \textit{orchestrator}. The orchestrator can employ the compute resources of the entire network to perform large parallel computation. The other computers in the network which perform the computation are called \textit{workers}. Since the application of this project's implementation is parallel learning, its focus will be on centralized networks.

    A centralized distributed computing network can be modelled mathematically using the notion of a network that was established in ~\autoref{sec:mathematical-description}. Denoting a collection of $n$ computers as $\mathbf{V} = (v_1, v_2, ..., v_n)$ with computation rates $\mathbf{R} = (r_1, r_2, ..., r_n)$ and price per processor-hour $\mathbf{P} = (p_1, p_2, ..., p_n)$, each $v_i$ can be thought of as a computer with a computation rate $r_i$ and a price to employ of $p_i$. In a \textit{heterogeneous} computer network, computational capacity and price of compute will vary from machine to machine.

    The structure of the distributed computing network is represented as the directed graph $\mathbf{G} = (\mathbf{V}, \mathbf{A})$. Because in a centralized network only the orchestrator can command processing power from the other computers, the adjacency matrix $\mathbf{A}$ is zero-valued for any edge which does not connect to the orchestrator node. In other words, there are no connections between any two computers that are not the orchestrator for the purposes of distributed computing. This feature is in addition to the other assumptions made for $\mathbf{A}$ in~\autoref{sec:mathematical-description}.

    When the orchestrator uses the network to compute in parallel, it is deploying a job denoted $J = (f,D)$ which consists of a function $f$ to be computed on a dataset $D \subseteq \mathbb{X}$. Here $\mathbb{X}$ is the infinite set of all possible input datasets. The orchestrator deploys a partition of $D$ to each employed worker along with copies of the work function $f$ for computation. The workers each return their computed results to the orchestrator, and these are the constituents of the result set $Y$.

    In distributed computing, it is in the best interest of the network to maximize the number of jobs that can be sent over the network and minimize the price to use the network. Therefore, optimizing for low run-time and cost of computing is desirable.

    By modelling the distributed computing network with the mathematical definition for a network, the run time and cost can be defined using the \textit{completion time} and \textit{resource price} functions, respectively. Given a job $J = (f,D)$ which is to be sent to the network and computed in parallel, one can minimize these functions regarding all variations of partitions of $D$ and their allocations to workers. If this is done before making use of the network, jobs can then be deployed strategically to minimize for these values and maximize the network's efficiency.

    \section{Triple Bottom Line}

    TODO: This was copied from proposal report. Should revise for concision and/or only include application stuff that is strictly related to the above description. Also should consider moving TBL stuff to chapter 11?

    %     - Helping research progress faster
    %     - Privacy concerns (e.g., fingerprinting)
    The suboptimal allocation of resources in edge computing will lower its overall performance, decreasing its effectiveness as an alternative to traditional computing and potentially making it a worse alternative. Thus, optimizing it is a prudent step in making it a preferable computing alternative for helping advance industry processes and the pace of research. But the optimization requires gathering information regarding the computational capabilities of a person's device, which can raise privacy concerns. It is a similar problem observed in device fingerprinting where user's privacy is intruded upon by unwanted tracking~\cite{laperdrix_browser_2020}. As such, it is imperative that when collecting statistics about devices working on the network, the information collected adheres to Canadaâ€™s federal private-sector privacy law, the \textit{Personal Information Protection and Electronic Documents Act} (PIPEDA)~\cite{noauthor_privacy_nodate}.

    % - Lowered costs for computing
    % - Electricity costs
    % - Economy of working as a node (being paid due to minimum working wage)
    % - Edge computing vs traditional Cloud computing e.g., AWS server farms (ref)
    % - Less ewaste by using computing power of already existing devices
    Suboptimally allocating resources in edge computing can also result in increased financial and environmental costs for people using the edge computing network through increased energy usage. In DL, the low performance caused by the limitations of the weakest worker results in a large amount of idle time on the orchestrator. Since idle or underutilized servers still use between 50-65 percent of the energy of fully utilized servers~\cite{noauthor_triple_nodate}, minimizing this time through reducing the impact of the weakest worker will result in increased energy efficiencies that will reduce the costs associated with wasted energy.
\end{document}
