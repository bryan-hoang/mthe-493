\providecommand{\main}{..}
\documentclass[../mthe-493-final-project.tex]{subfiles}

% Introduction: Introduce the project, the problem its solving, its relevance, the current state, and our goal. More contextual than technical.

\begin{document}
    \chapter{Introduction}
    \label{ch:introduction}

    TODO: This was copied from proposal report. Should review/revise as needed.

    Edge computing is an emerging paradigm for IoT and data analytics systems. It is a method of performing computationally heavy tasks that does the work on end-user devices. These systems can scale well and can drastically simplify infrastructure provisioning. Additionally, edge computing minimizes that amount of data required to be sent back and forth, since the work is done right on the device. With global industry embracing smart architecture and IoT processes, the edge-computing market is predicted to grow to \$43.4 billion by 2027, by which time 75\% of enterprise-generated data will be created and processed at the edge.~\cite{noauthor_edge_2020}

    In many of its applications, such as image recognition, forecasting, and clustering, Machine Learning (ML) has outperformed existing techniques. This has motivated the need to deploy computationally dense ML models on end devices. One area of ML where substantial momentum has been gained is in Distributed Learning (DL). This is the method of training of a ML model by aggregating many local models which are trained on edge devices. Federated learning (FL) is similar to distributed learning, but stores the data on edge devices. In this case, the edge nodes are typically where training data is collected, such as a sensor on a smartphone. The focus of this project will be parallel learning (PL), where data begins in a central orchestrator and distributes it to edge devices for training.

    When attempting to optimally configure a distributed computing network for PL, one must consider two important variables, network costs and computational capabilities. When there are differences in the network costs and computational capabilities of edge devices, this is called system heterogeneity, and it makes creating an optimal configuration substantially more difficult. If heterogeneity is not addressed, distributed learning performance is limited by its weakest edge or slowest connection. There are two approaches to circumvent heterogeneity. One can either vary the volume of data sent to each edge, or vary the number of local training iterations performed on each edge. The latter is often less accurate due to a discrepancy created in edge gradients that is referred to as 'staleness'. This project will perform experiments with both methods.
    
\end{document}